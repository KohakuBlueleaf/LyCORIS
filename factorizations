lora_te_text_model_encoder_layers_0_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_0_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_0_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_0_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_0_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_0_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_1_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_1_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_1_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_1_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_1_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_1_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_2_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_2_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_2_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_2_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_2_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_2_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_3_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_3_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_3_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_3_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_3_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_3_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_4_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_4_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_4_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_4_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_4_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_4_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_5_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_5_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_5_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_5_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_5_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_5_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_6_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_6_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_6_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_6_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_6_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_6_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_7_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_7_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_7_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_7_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_7_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_7_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_8_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_8_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_8_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_8_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_8_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_8_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_9_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_9_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_9_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_9_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_9_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_9_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_10_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_10_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_10_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_10_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_10_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_10_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_te_text_model_encoder_layers_11_self_attn_k_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_11_self_attn_v_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_11_self_attn_q_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_11_self_attn_out_proj : (768, 768) -> (24, 24)⊗(32, 32)
lora_te_text_model_encoder_layers_11_mlp_fc1 : (768, 3072) -> (24, 48)⊗(32, 64)
lora_te_text_model_encoder_layers_11_mlp_fc2 : (3072, 768) -> (48, 24)⊗(64, 32)
lora_unet_down_blocks_0_attentions_0_proj_in : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj : (320, 2560) -> (16, 40)⊗(20, 64)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2 : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_0_proj_out : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_proj_in : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj : (320, 2560) -> (16, 40)⊗(20, 64)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2 : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_attentions_1_proj_out : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_resnets_0_conv1 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_resnets_0_time_emb_proj : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_down_blocks_0_resnets_0_conv2 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_resnets_1_conv1 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_resnets_1_time_emb_proj : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_down_blocks_0_resnets_1_conv2 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_0_downsamplers_0_conv : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_down_blocks_1_attentions_0_proj_in : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj : (640, 5120) -> (20, 64)⊗(32, 80)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 : (2560, 640) -> (40, 20)⊗(64, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_0_proj_out : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_proj_in : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj : (640, 5120) -> (20, 64)⊗(32, 80)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 : (2560, 640) -> (40, 20)⊗(64, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_attentions_1_proj_out : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_resnets_0_conv1 : (320, 640) -> (16, 20)⊗(20, 32)
lora_unet_down_blocks_1_resnets_0_time_emb_proj : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_down_blocks_1_resnets_0_conv2 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_resnets_0_conv_shortcut : (320, 640) -> (16, 20)⊗(20, 32)
lora_unet_down_blocks_1_resnets_1_conv1 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_resnets_1_time_emb_proj : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_down_blocks_1_resnets_1_conv2 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_1_downsamplers_0_conv : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_down_blocks_2_attentions_0_proj_in : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj : (1280, 10240) -> (32, 80)⊗(40, 128)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 : (5120, 1280) -> (64, 32)⊗(80, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_0_proj_out : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_proj_in : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj : (1280, 10240) -> (32, 80)⊗(40, 128)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 : (5120, 1280) -> (64, 32)⊗(80, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_attentions_1_proj_out : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_resnets_0_conv1 : (640, 1280) -> (20, 32)⊗(32, 40)
lora_unet_down_blocks_2_resnets_0_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_resnets_0_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_resnets_0_conv_shortcut : (640, 1280) -> (20, 32)⊗(32, 40)
lora_unet_down_blocks_2_resnets_1_conv1 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_resnets_1_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_resnets_1_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_2_downsamplers_0_conv : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_3_resnets_0_conv1 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_3_resnets_0_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_3_resnets_0_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_3_resnets_1_conv1 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_3_resnets_1_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_down_blocks_3_resnets_1_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_0_conv1 : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_0_resnets_0_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_0_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_0_conv_shortcut : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_0_resnets_1_conv1 : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_0_resnets_1_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_1_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_1_conv_shortcut : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_0_resnets_2_conv1 : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_0_resnets_2_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_2_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_0_resnets_2_conv_shortcut : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_0_upsamplers_0_conv : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_proj_in : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj : (1280, 10240) -> (32, 80)⊗(40, 128)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 : (5120, 1280) -> (64, 32)⊗(80, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_0_proj_out : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_proj_in : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj : (1280, 10240) -> (32, 80)⊗(40, 128)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 : (5120, 1280) -> (64, 32)⊗(80, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_1_proj_out : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_proj_in : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj : (1280, 10240) -> (32, 80)⊗(40, 128)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2 : (5120, 1280) -> (64, 32)⊗(80, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_attentions_2_proj_out : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_0_conv1 : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_1_resnets_0_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_0_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_0_conv_shortcut : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_1_resnets_1_conv1 : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_1_resnets_1_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_1_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_1_conv_shortcut : (2560, 1280) -> (40, 32)⊗(64, 40)
lora_unet_up_blocks_1_resnets_2_conv1 : (1920, 1280) -> (32, 32)⊗(60, 40)
lora_unet_up_blocks_1_resnets_2_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_2_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_1_resnets_2_conv_shortcut : (1920, 1280) -> (32, 32)⊗(60, 40)
lora_unet_up_blocks_1_upsamplers_0_conv : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_up_blocks_2_attentions_0_proj_in : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj : (640, 5120) -> (20, 64)⊗(32, 80)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 : (2560, 640) -> (40, 20)⊗(64, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_0_proj_out : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_proj_in : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj : (640, 5120) -> (20, 64)⊗(32, 80)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 : (2560, 640) -> (40, 20)⊗(64, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_1_proj_out : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_proj_in : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj : (640, 5120) -> (20, 64)⊗(32, 80)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2 : (2560, 640) -> (40, 20)⊗(64, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v : (768, 640) -> (24, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_attentions_2_proj_out : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_resnets_0_conv1 : (1920, 640) -> (32, 20)⊗(60, 32)
lora_unet_up_blocks_2_resnets_0_time_emb_proj : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_up_blocks_2_resnets_0_conv2 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_resnets_0_conv_shortcut : (1920, 640) -> (32, 20)⊗(60, 32)
lora_unet_up_blocks_2_resnets_1_conv1 : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_up_blocks_2_resnets_1_time_emb_proj : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_up_blocks_2_resnets_1_conv2 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_resnets_1_conv_shortcut : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_up_blocks_2_resnets_2_conv1 : (960, 640) -> (30, 20)⊗(32, 32)
lora_unet_up_blocks_2_resnets_2_time_emb_proj : (1280, 640) -> (32, 20)⊗(40, 32)
lora_unet_up_blocks_2_resnets_2_conv2 : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_2_resnets_2_conv_shortcut : (960, 640) -> (30, 20)⊗(32, 32)
lora_unet_up_blocks_2_upsamplers_0_conv : (640, 640) -> (20, 20)⊗(32, 32)
lora_unet_up_blocks_3_attentions_0_proj_in : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj : (320, 2560) -> (16, 40)⊗(20, 64)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2 : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_0_proj_out : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_proj_in : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj : (320, 2560) -> (16, 40)⊗(20, 64)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2 : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_1_proj_out : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_proj_in : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj : (320, 2560) -> (16, 40)⊗(20, 64)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2 : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v : (768, 320) -> (24, 16)⊗(32, 20)
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_attentions_2_proj_out : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_resnets_0_conv1 : (960, 320) -> (30, 16)⊗(32, 20)
lora_unet_up_blocks_3_resnets_0_time_emb_proj : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_up_blocks_3_resnets_0_conv2 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_resnets_0_conv_shortcut : (960, 320) -> (30, 16)⊗(32, 20)
lora_unet_up_blocks_3_resnets_1_conv1 : (640, 320) -> (20, 16)⊗(32, 20)
lora_unet_up_blocks_3_resnets_1_time_emb_proj : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_up_blocks_3_resnets_1_conv2 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_resnets_1_conv_shortcut : (640, 320) -> (20, 16)⊗(32, 20)
lora_unet_up_blocks_3_resnets_2_conv1 : (640, 320) -> (20, 16)⊗(32, 20)
lora_unet_up_blocks_3_resnets_2_time_emb_proj : (1280, 320) -> (32, 16)⊗(40, 20)
lora_unet_up_blocks_3_resnets_2_conv2 : (320, 320) -> (16, 16)⊗(20, 20)
lora_unet_up_blocks_3_resnets_2_conv_shortcut : (640, 320) -> (20, 16)⊗(32, 20)
lora_unet_mid_block_attentions_0_proj_in : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj : (1280, 10240) -> (32, 80)⊗(40, 128)
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2 : (5120, 1280) -> (64, 32)⊗(80, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v : (768, 1280) -> (24, 32)⊗(32, 40)
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_attentions_0_proj_out : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_resnets_0_conv1 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_resnets_0_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_resnets_0_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_resnets_1_conv1 : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_resnets_1_time_emb_proj : (1280, 1280) -> (32, 32)⊗(40, 40)
lora_unet_mid_block_resnets_1_conv2 : (1280, 1280) -> (32, 32)⊗(40, 40)
